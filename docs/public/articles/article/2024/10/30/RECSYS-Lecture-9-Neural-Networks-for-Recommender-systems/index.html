<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      RECSYS Lecture 9 - Neural Networks for Recommender systems &middot; JustAMonkey
    
  </title>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,400italic|Source+Code+Pro:400,700" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.min.css" type="text/css">
  <link rel="stylesheet" href="/css/font-computer.min.css" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css" type="text/css">

  <link rel="icon" type="image/png" sizes="192x192" href="/favicon.png">
  <link rel="shortcut icon" href="/favicon.ico">
  <link rel="apple-touch-icon-precomposed" sizes="152x152" href="/apple-touch-icon.png">
  
  <link rel="alternate" type="application/atom+xml" title="JustAMonkey" href="/atom.xml">
<!--
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']],
        displayMath: [ ['$$','$$']],
        processEscapes: true
      }
    });
  </script>
-->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    }
  });
</script>
  
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  
</head>


  <body>
    <div class="image-container">
      <a class="justamonkey" href="/"><img src="/assets/images/justamonkey-high-resolution-logo-transparent.png" alt="Logo del sito"></a>
    </div>
    <!-- Social Icons -->
    <div class="social-icons">
      <a href="mailto:sebastianomorson@outlook.com"><i class="fa fa-envelope"></i></a>
      <a href="https://github.com/SebastianoMorson"><i class="fa fa-github"></i></a>
      <a href="https://www.linkedin.com/in/sebastiano-morson-34a825221/?originalSubdomain=it"><i class="fa fa-linkedin"></i></a>
    </div>
    <nav class="nav-main">
      <ul>
        <li class="hvr-underline-reveal"><a href="/whoami/">Whoami</a></li>
	      <!--<li class="logo"><a class="hvr-ripple-out" href="/">H</a></li> -->
          <li class="hvr-underline-reveal"><a href="/blog/">Projects</a></li>
	      <li class="hvr-underline-reveal"><a href="/articles/">Articles</a></li>
        <li class="hvr-underline-reveal"><a href="/hobbies/">Hobbies</a></li>
	
      </ul>
    </nav>

    <div class="container content">
      <main>
        <article class="post">
  <h1 class="post-title">RECSYS Lecture 9 - Neural Networks for Recommender systems</h1>
  <time datetime="2024-10-30T00:00:00+01:00" class="post-date">30 Oct 2024</time>
  <!--more-->

<p><img src="https://static.wixstatic.com/media/4959fd_a17b017a6ae84511ac56d00b34230941~mv2.png/v1/fill/w_280,h_190,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/4959fd_a17b017a6ae84511ac56d00b34230941~mv2.png" alt="" /></p>

<p>I metodi che abbiamo visto fin’ora andavano bene finchè i dataset erano piccoli, ma quando i dataset diventano grandi e si vogliono catturare legami non lineari è necessario chiamare la cavalleria: le reti neurali.</p>

<p>Facciamo un piccolo recap di come funziona una rete neurale:</p>

<ul>
  <li>
    <p>abbiamo dei nodi di input</p>
  </li>
  <li>
    <p>abbiamo dei nodi intermedi</p>
  </li>
  <li>
    <p>abbiamo dei nodi di output</p>
  </li>
</ul>

<p>In un sistema di raccomandazione, ci si aspetta che ci sia una relazione tra gli item valutati dagli utenti e gli item che saranno più propensi a considerare il futuro. In questo senso le reti ricorrenti sono particolarmente adatte per risolvere il problema di eseguire previsioni basate sulle sequenze di items con cui l’utente ha interagito.</p>

<h2 id="recurrent-neural-networks-rnns">Recurrent Neural Networks (RNNs)</h2>

<p>Le reti ricorrenti sono reti che hanno questa forma qua</p>

<p><img src="https://static.wixstatic.com/media/4959fd_c2cf60f547394bda858f050cbccab52a~mv2.png/v1/fill/w_592,h_218,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/4959fd_c2cf60f547394bda858f050cbccab52a~mv2.png" alt="" /></p>

<p>come si può osservare ciascun input nella sequenza x1,x2,…xn permette di generare una predizione che dipende dalla previsione precedente.</p>

<p>Il problema principale di questi modelli è che più la sequenza degli items è lunga, più l’effetto “memoria” del modello tende a svanire, perciò gli elementi iniziali della sequenza contribuiscono in maniera minore alle predizioni dei livelli successivi.</p>

<p>Per questo motivo sono stati pensati due ulteriori modelli che fanno uso di funzioni di gate per migliorare la propagazione delle informazioni dei layer precedenti su quelli successivi. Questi due modelli sono GRU e LSTM</p>

<p>###</p>

<p><img src="https://static.wixstatic.com/media/4959fd_5d9397f703b84e3bb31aa11a6f0283a3~mv2.png/v1/fill/w_592,h_171,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/4959fd_5d9397f703b84e3bb31aa11a6f0283a3~mv2.png" alt="" /></p>

<h3 id="gated-recurrent-unit-gru">Gated Recurrent Unit (GRU)</h3>

<p><img src="https://static.wixstatic.com/media/4959fd_632ae74fac8a4fa09a1e529fcc72c6ab~mv2.png/v1/fill/w_314,h_195,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/4959fd_632ae74fac8a4fa09a1e529fcc72c6ab~mv2.png" alt="" /></p>

<h3 id="long-short-term-memory-lstm">Long Short Term Memory (LSTM)</h3>

<p><img src="https://static.wixstatic.com/media/4959fd_bb5adf0154fc483c802b25cf3fe8ca6a~mv2.png/v1/fill/w_404,h_205,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/4959fd_bb5adf0154fc483c802b25cf3fe8ca6a~mv2.png" alt="" /></p>

<p>Il problema di queste reti è che sono complicate e costose.</p>

<p>Una GRU o una LSTM infatti ha bisogno di molti dati di training e di molte risorse di calcolo perchè</p>

<ol>
  <li>
    <p>ogni input è relazionato con il precedente</p>
  </li>
  <li>
    <p>a causa del punto 1 non siamo in grado di parallelizzare le computazioni</p>
  </li>
</ol>

<p>Tutti questi problemi sono risolti dai transformers.</p>

<h2 id="transformer">Transformer</h2>

<p>I transformer si basano sul concetto di self-attention, ossia data una sequenza di input, ogni elemento della sequenza considera la sua relazione con gli altri elementi della sequenza in modo da individuare quali siano gli elementi chiave per rappresentare la sequenza.</p>

<p>La loro struttura è la seguente</p>

<p><img src="https://static.wixstatic.com/media/4959fd_d27c32ec2897496287eb52a4e4fa722e~mv2.png/v1/fill/w_592,h_341,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/4959fd_d27c32ec2897496287eb52a4e4fa722e~mv2.png" alt="" /></p>

<p>Ma perchè li introduciamo nel contesto del collaborative filtering? Perchè funzionano meglio delle LSTM e di GRU. Il fatto è che riescono a catturare meglio le relazioni tra gli elementi di input nel lungo termine e contesti globali, oltre al fatto che permettono di paralellizzare la computazione, il che è utilissimo.</p>

<p>Tutto molto bello, ma rimangono alcuni problemi classicissimi:</p>

<ol>
  <li>
    <p>cold start problem</p>
  </li>
  <li>
    <p>gestire la sparsità delle matrici delle valutazioni</p>
  </li>
  <li>
    <p>gestire le relazioni complesse tra gli items</p>
  </li>
</ol>

<p>Possibile che le reti neurali non possano aiutarci?</p>

<p>Eccerto che possono farlo.</p>

<h2 id="neural-collaborative-filtering-ncf">Neural Collaborative Filtering (NCF)</h2>

<p>La prima proposta è il neural collaborative filtering.</p>

<p>Il problema che si cerca di risolvere è quello che avevamo visto quando abbiamo parlato di matrix factorization: l’inner product solitamente cattura correlazioni lineari tra le due matrici latenti, ma noi vogliamo cercare di catturare anche le relazioni profonde.</p>

<p>Per catturare relazioni profonde potremmo agire in due maniere:</p>

<ul>
  <li>
    <p>aumentiamo il numero di fattori latenti</p>

    <ul>
      <li>
        <p>ma questo può portare ad overfitting essendo che potrei non aver abbastanza dati per spiegare le features. Ad esempio assumiamo di avere 10 utenti e 5 cani da poter raccomandare. Potrei voler applicare la matrice di fattorizzazione e ottenere due matrici latenti per gli utenti e per i cani (che sono gli items). Quindi potrei scegliere di avere 3 fattori latenti:</p>

        <ul>
          <li>
            <p>lunghezza pelo</p>
          </li>
          <li>
            <p>colore pelo</p>
          </li>
          <li>
            <p>peso da adulto</p>
          </li>
        </ul>

        <p>oppure ne protrei avere 10</p>

        <ul>
          <li>
            <p>lunghezza pelo</p>
          </li>
          <li>
            <p>colore pelo</p>
          </li>
          <li>
            <p>peso da adulto</p>
          </li>
          <li>
            <p>lunghezza</p>
          </li>
          <li>
            <p>altezza</p>
          </li>
          <li>
            <p>aggressività</p>
          </li>
          <li>
            <p>pelo riccio</p>
          </li>
          <li>
            <p>pelo liscio</p>
          </li>
          <li>
            <p>pelo crespo</p>
          </li>
          <li>
            <p>pelo morbido</p>
          </li>
        </ul>

        <p>Il problema nel primo caso è che potrei essere troppo generico con sole 3 caratteristiche latenti perchè i 5 cani potrebbero essere tutti molto simili tra loro in queste caratteristiche, mentre con 10 features potrei essere troppo specifico e ottenere una rappresentazione troppo poco generica di un elemento che piace al nostro utente.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>usiamo una deep neural network per catturare le correlazioni non lineari a partire dalle rappresentazioni latenti.</p>
  </li>
</ul>

<p>L’ultima idea è quella usata dal neural collaborative filtering. Intendiamo usare le deep neural networks per identificare comportamenti non lineari derivati dalla matrice di fattorizzazione generata a partire dalla matrice dei ratings.</p>

<p>In un colpo riusciamo a ridurre la dimensione della matrice delle valutazioni e ottenere un previsioni più accurate grazie allo studio degli aspetti non lineari nelle relazioni tra gli items.</p>

<p>Usiamo quindi un modello strutturato in questo modo</p>

<p><img src="https://static.wixstatic.com/media/4959fd_c0b40db59e624364b3af97420d2ea9f2~mv2.png/v1/fill/w_512,h_298,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/4959fd_c0b40db59e624364b3af97420d2ea9f2~mv2.png" alt="" /></p>

<p>Di base cosa si fa: si prende il vettore dei fattori latenti dell’utente e il vettore dei fattori latenti degli items e si concatenano assieme prima di darli in pasto a una rete neurale profonda.</p>

<h2 id="deep-general-matrix-factorization-gmf">Deep General Matrix Factorization (GMF)</h2>

<p>La deep general matrix factorization è una sorta di matrix factorization sotto steroidi, infatti si è visto che performa meglio della matrix factorization tradizionale soprattutto quando si ha a che fare con matrici fortemente sparse e dimensionalità dei dati molto alte.</p>

<h2 id="neumf-neural-matrix-factorization">NeuMF (Neural Matrix Factorization)</h2>

<p>Il problema del Neural Collaborative Filtering è che tende a perdere il contributo dato dalle relazioni lineari tra le rappresentazioni latenti di user e items. Per questo in NeuMF l’idea è quella di usare in combo le reti neurali e la matrix factorization con l’obiettivo di catturare sia i fattori latenti lineari, sia i comportamenti non lineari. Il modello enseamble che si ottiene unendo GMF e MLP seguente</p>

<p><img src="https://static.wixstatic.com/media/4959fd_ada68c9a9ff0473b9553b4419153e0f1~mv2.png/v1/fill/w_592,h_282,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/4959fd_ada68c9a9ff0473b9553b4419153e0f1~mv2.png" alt="" /></p>

<p>Di base cosa si fa: si prende il vettore dei fattori latenti dell’utente e il vettore dei fattori latenti degli items e si concatenano assieme. Dopodichè si fattorizza nuovamente la rappresentazione della matrice di valutazione e si considerano due nuove matrici di dimensionalità inferiore che vengono dati in pasto a una rete neurale profonda alla ricerca di legami non lineari.</p>

<p>Qualcuno potrebbe obiettare dicendo “eh, ma non bastava usare usa singola rappresentazione GMF o MLP invece che due rappresentazioni separate sia per gli items che per gli users?”. Risposta: si è visto che usare un ebbending layer condiviso limita le capacità del modello.</p>

<h2 id="bert4rec">BERT4Rec</h2>

<p>Ed ecco a voi l’ultimo, ma non ultimo modello presentato in questa lezione sulle reti neurali applicate ai recommender systems.</p>

<p>Si può immaginare la relazione tra RNN, NeuMF e Bert4Rec in questo modo:</p>

<p><img src="https://static.wixstatic.com/media/4959fd_17f2350a3ae94feebb3c18e91e2cf626~mv2.png/v1/fill/w_343,h_213,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/4959fd_17f2350a3ae94feebb3c18e91e2cf626~mv2.png" alt="" /></p>

<p>La struttura di Bert4Rec è questa qua</p>

<p><img src="https://static.wixstatic.com/media/4959fd_ed99bf0a2f0445f293188dedbb5dd726~mv2.png/v1/fill/w_592,h_498,al_c,lg_1,q_85,enc_auto/4959fd_ed99bf0a2f0445f293188dedbb5dd726~mv2.png" alt="" /></p>

<p>Nella prima parte, abbiamo un layer di embedding che serve per creare una rappresentazione densa dei vettori di input.</p>

<p>Un buon compromesso di masking è tra il 10% e il 20% degli elementi. Questo parametro varia in base al dataset utilizzato.</p>

<p>Il layer fully connected è quello che permette al modello di catturare le relazioni non lineari nella sequenza di input e trasmettere la conoscenza passata ai transformers successivi.</p>

<p>L’immagine sottostante è un altro modo per vedere il layer dei transformers</p>

<p><img src="https://static.wixstatic.com/media/4959fd_09e6cbcce0f343dca62ab6f0472126b0~mv2.png/v1/fill/w_425,h_442,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/4959fd_09e6cbcce0f343dca62ab6f0472126b0~mv2.png" alt="" /></p>

<p>Ricordiamoci che ogni transformer segue questo modello</p>

<p><img src="https://static.wixstatic.com/media/4959fd_da542c67c2c842cbb9c1e88958f154f4~mv2.png/v1/fill/w_592,h_358,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/4959fd_da542c67c2c842cbb9c1e88958f154f4~mv2.png" alt="" /></p>

<h3 id="possibili-domande">Possibili domande</h3>

<ol>
  <li>
    <p>What are the key components of a Long Short-Term Memory (LSTM) network and their functions?</p>
  </li>
  <li>
    <p>How do neural networks enhance collaborative filtering in recommender systems?</p>
  </li>
  <li>
    <p>What is the purpose of an autoencoder in unsupervised learning?</p>
  </li>
  <li>
    <p>What is NeuMF and how does it function in recommender systems?</p>
  </li>
  <li>
    <p>What are the main advantages of using neural networks for recommender systems?</p>
  </li>
  <li>
    <p>Explain the concept of weight sharing in convolutional neural networks and its significance in reducing the number of parameters.</p>
  </li>
  <li>
    <p>Discuss the role of regularization techniques in neural networks and provide examples of common methods used to prevent overfitting.</p>
  </li>
  <li>
    <p>What are the main components of a neural network and how do they interact to perform tasks such as classification or regression?</p>
  </li>
  <li>
    <p>Describe the architecture and functionality of the NeuMF model in the context of recommender systems.</p>
  </li>
  <li>
    <p>What are the advantages of using Long Short-Term Memory (LSTM) networks over traditional recurrent neural networks (RNNs)?</p>
  </li>
  <li>
    <p>What is the primary function of the pooling layer in a convolutional neural network?</p>
  </li>
</ol>

<p>A) To apply convolution operations</p>

<p>B) To reduce spatial dimensions</p>

<p>C) To connect neurons in a fully connected layer</p>

<p>D) To learn local patterns</p>

<ol>
  <li>What does the input gate in an LSTM control?</li>
</ol>

<p>A) The flow of output from the memory cell</p>

<p>B) The retention of memory</p>

<p>C) The flow of input into the memory cell</p>

<p>D) The calculation of the hidden state</p>

<ol>
  <li>What is the main goal of an autoencoder in unsupervised learning?</li>
</ol>

<p>A) To classify input data</p>

<p>B) To reconstruct input from a latent representation</p>

<p>C) To predict user preferences</p>

<p>D) To learn linear feature representations</p>

<ol>
  <li>What is the purpose of the forget gate in an LSTM?</li>
</ol>

<p>A) To control the flow of input</p>

<p>B) To manage memory retention</p>

<p>C) To calculate the hidden state</p>

<p>D) To shift the activation function</p>

<ol>
  <li>What does NeuMF combine to enhance recommender systems?</li>
</ol>

<p>A) Matrix factorization and decision trees</p>

<p>B) Matrix factorization and deep neural networks</p>

<p>C) Traditional CF models and linear regression</p>

<p>D) Deep learning and clustering techniques</p>

<ol>
  <li>Convolutional neural networks (CNNs) are specialized for processing sequential data.</li>
</ol>

<ul>
  <li>
    <p>True</p>
  </li>
  <li>
    <p>False</p>
  </li>
</ul>

<ol>
  <li>The main purpose of the pooling layer in a CNN is to increase the spatial dimensions of the input data.</li>
</ol>

<ul>
  <li>
    <p>True</p>
  </li>
  <li>
    <p>False</p>
  </li>
</ul>

<ol>
  <li>NeuMF stands for Neural Matrix Factorization and combines matrix factorization with deep neural networks.</li>
</ol>

<ul>
  <li>
    <p>True</p>
  </li>
  <li>
    <p>False</p>
  </li>
</ul>

<ol>
  <li>LSTMs are designed to address the vanishing gradient problem in traditional RNNs.</li>
</ol>

<ul>
  <li>
    <p>True</p>
  </li>
  <li>
    <p>False</p>
  </li>
</ul>

<ol>
  <li>Autoencoders are used for supervised learning tasks such as classification.</li>
</ol>

<ul>
  <li>
    <p>True</p>
  </li>
  <li>
    <p>False</p>
  </li>
</ul>

<ol>
  <li>
    <p>What are the key components of a Long Short-Term Memory (LSTM) network and their functions?</p>
  </li>
  <li>
    <p>Explain the concept of weight sharing in convolutional neural networks and its benefits.</p>
  </li>
  <li>
    <p>Describe the role of regularization in neural networks and list some common techniques used.</p>
  </li>
  <li>
    <p>What is the purpose of autoencoders in unsupervised learning, and what are their two main components?</p>
  </li>
  <li>
    <p>How do neural networks enhance collaborative filtering in recommender systems?</p>
  </li>
  <li>
    <p>What are the main advantages of using neural networks for recommender systems compared to traditional methods?</p>
  </li>
  <li>
    <p>What is the function of the fully connected layer in a convolutional neural network?</p>
  </li>
  <li>
    <p>What is the significance of activation functions in neural networks?</p>
  </li>
  <li>
    <p>Explain the concept of memory cells in Long Short-Term Memory (LSTM) networks.</p>
  </li>
  <li>
    <p>What is the role of the output layer in the NeuMF model for recommender systems?</p>
  </li>
  <li>
    <p>What are the key components of a Long Short-Term Memory (LSTM) network, and how do they contribute to its ability to learn long-term dependencies?</p>
  </li>
  <li>
    <p>Explain the role of regularization in neural networks and describe some common techniques used to prevent overfitting.</p>
  </li>
  <li>
    <p>Describe the architecture of NeuMF and how it integrates matrix factorization with neural networks.</p>
  </li>
  <li>
    <p>What are the advantages of using neural networks in recommender systems compared to traditional collaborative filtering methods?</p>
  </li>
  <li>
    <p>What is the purpose of the encoder and decoder in an autoencoder, and how do they work together to achieve the goal of reconstruction?</p>
  </li>
  <li>
    <p>What is the significance of weight sharing in convolutional neural networks, and how does it impact the model’s performance?</p>
  </li>
  <li>
    <p>How do neural networks enhance collaborative filtering in recommender systems compared to traditional methods?</p>
  </li>
  <li>
    <p>Explain the role of activation functions in neural networks and why they are essential for model performance.</p>
  </li>
  <li>
    <p>What are the main components of a Long Short-Term Memory (LSTM) network, and how do they work together to address the vanishing gradient problem?</p>
  </li>
  <li>
    <p>Describe the optimization objective of autoencoders and the significance of minimizing reconstruction error.</p>
  </li>
</ol>

<ul>
  <li><a href="https://sebastianomorson.wixsite.com/sebastiano-morson/blog/tags/artificial-intelligence"></a></li>
</ul>

</article>

      </main>
    </div>

    <footer class="footer">
      <small>
          <span class="copyright"><i class="fa fa-copyright"></i> 2024-<time datetime="2025-01-06T11:35:21+01:00">2025</time> </span> &middot;
          <span>Powered by <a href="http://jekyllrb.com/">Jekyll</a></span>
      </small>

    </footer>

</html>
