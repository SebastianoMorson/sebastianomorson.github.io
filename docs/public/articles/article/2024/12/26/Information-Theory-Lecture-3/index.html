<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Information Theory - Lecture 3 &middot; JustAMonkey
    
  </title>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,400italic|Source+Code+Pro:400,700" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.min.css" type="text/css">
  <link rel="stylesheet" href="/css/font-computer.min.css" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css" type="text/css">

  <link rel="icon" type="image/png" sizes="192x192" href="/favicon.png">
  <link rel="shortcut icon" href="/favicon.ico">
  <link rel="apple-touch-icon-precomposed" sizes="152x152" href="/apple-touch-icon.png">
  
  <link rel="alternate" type="application/atom+xml" title="JustAMonkey" href="/atom.xml">
<!--
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']],
        displayMath: [ ['$$','$$']],
        processEscapes: true
      }
    });
  </script>
-->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    }
  });
</script>
  
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  
</head>


  <body>
    <div class="image-container">
      <a class="justamonkey" href="/"><img src="/assets/images/justamonkey-high-resolution-logo-transparent.png" alt="Logo del sito"></a>
    </div>
    <!-- Social Icons -->
    <div class="social-icons">
      <a href="mailto:sebastianomorson@outlook.com"><i class="fa fa-envelope"></i></a>
      <a href="https://github.com/SebastianoMorson"><i class="fa fa-github"></i></a>
      <a href="https://www.linkedin.com/in/sebastiano-morson-34a825221/?originalSubdomain=it"><i class="fa fa-linkedin"></i></a>
    </div>
    <nav class="nav-main">
      <ul>
        <li class="hvr-underline-reveal"><a href="/whoami/">Whoami</a></li>
	      <!--<li class="logo"><a class="hvr-ripple-out" href="/">H</a></li> -->
          <li class="hvr-underline-reveal"><a href="/blog/">Projects</a></li>
	      <li class="hvr-underline-reveal"><a href="/articles/">Articles</a></li>
        <li class="hvr-underline-reveal"><a href="/hobbies/">Hobbies</a></li>
	
      </ul>
    </nav>

    <div class="container content">
      <main>
        <article class="post">
  <h1 class="post-title">Information Theory - Lecture 3</h1>
  <time datetime="2024-12-26T00:00:00+01:00" class="post-date">26 Dec 2024</time>
  <!--more-->
<h2 id="theorem-direct">Theorem Direct</h2>
<p>Se $l_{1},l_{2},\dots,l_{k}$ sono tali che $\sum_{i=1}^k D^{-l_{i}} \le 1$, allora esiste un prefix code dove le lunghezze degli encoding sono $l_{1},l_{2},\dots,l_{k}$</p>

<p><strong><em>Dimostrazione</em></strong>
La dimostrazione si basa sullo stesso concetto della dimostrazione del [[2024-12-26-Information Theory Lecture 2#Proof for prefix codes|teorema di Kraft MacMillan per i prefix code ]]
![[Pasted image 20240307095131.png]]</p>
<h2 id="remark">Remark</h2>
<p>Ricordiamo la definizione di [[2024-12-26-Information Theory Lecture 2#Uniquely Decodable with delay 1|Uniquely Decodable]].
I prefix code <strong><em>hanno lo stesso potere dei U.D. codes</em></strong>.
Possiamo raggiungere gli stessi livelli di compressione e non abbiamo delay nel decoding usando i prefix codes.</p>

<h2 id="definition-of-el">Definition of EL</h2>
<p>Assumiamo</p>

\[\begin{aligned}
&amp;A\text{ primary alphabet}\\
&amp;B\text{ secondary alphabet} \\
&amp;\varphi:A^* \to B^*
\end{aligned}\]

<p>Allora</p>

<p>\(\begin{aligned}
EL(\varphi) &amp;= \sum_{i=1}^k p_{i}\cdot l(\varphi(a_{i})) \\
&amp;= \sum_{i=1}^k p_{i \cdot l_{i}}
\end{aligned}\)
Ossia, $EL(\varphi)$ rappresenta la lunghezza media delle stringhe del codice</p>
<h1 id="code-rate">Code Rate</h1>

<h2 id="compression-rate">Compression Rate</h2>

<h3 id="1-shannon-theorem-source-code-theorem">1° Shannon Theorem (Source Code Theorem)</h3>
<p>Se $\varphi$ è Uniquely Decodable allora</p>

\[EL(\varphi) \ge H_{D}(P)\]

<p>dove $H_D(P)$ è l’entropia di $P$ usando $log_D$</p>

<p>L’idea è che non si può esprimere più informazione dell’entropia che abbiamo.
Perciò il teorema di Shannon dimostra che l’entropia $H_D(P)$ è una misura teorica del numero di bit medio necessari a rappresentare l’informazione di una sorgente attraverso una codifica ottimale.</p>

<p><strong><em>Dimostrazione</em></strong></p>

<p><em>Tesi</em>: $EL(\varphi) - H_{D}(P) \ge 0$</p>

\[\begin{aligned}
EL(\varphi) - H_{D}(P) &amp;= \sum_{i=1}^k p_{i} \cdot l_{i} + \sum_{i=1}^k p_{i} \log_{D} p_{i} \\
&amp;= \sum_{i=1}^k p_{i} (l_{i}+\log_{D}p_{i}) \\
&amp;= \sum_{i=1}^k p_{i} \left(\log_{D}D^{l_{i}} + \log_{D} p_{i}\right) \\
&amp;= \frac{1}{\ln D } \cdot \sum_{i=1}^k p_{i} \ln(D^{l_{i}}\cdot p_{i})\\
&amp;= -\frac{1}{\ln D} \cdot \sum_{i=1}^k p_{i} \ln\left( \frac{1}{D^{l_{i}} \cdot p_{i}} \right) \\
&amp;\ge -\frac{1}{\ln D} \sum_{i=1}^k p_{i} \ln\left( \frac{1}{D^{l_{i}} \cdot p_{i}} - 1 \right) \\
&amp;\ge -\frac{1}{\ln D} \left( \sum_{i=1}^k p_{i} \cdot \frac{1}{D^{l_{i}}\cdot p_{i}} - \sum_{i=1}^k p_{i} \cdot 1 \right) \\
&amp;\ge -\frac{1}{\ln D}\left( \sum_{i=1}^k D^{-l_{i}} -1 \right) \\ \\
&amp;\text{Ma per il teorema di Kraft MacMillen}\\
&amp;\text{quello che sta dentro le parentesi è } \le 1\\ &amp;\text{Perciò:}
\\ \\
&amp;\ge 0

\end{aligned}\]

</article>

      </main>
    </div>

    <footer class="footer">
      <small>
          <span class="copyright"><i class="fa fa-copyright"></i> 2024-<time datetime="2025-01-06T11:35:21+01:00">2025</time> </span> &middot;
          <span>Powered by <a href="http://jekyllrb.com/">Jekyll</a></span>
      </small>

    </footer>

</html>
